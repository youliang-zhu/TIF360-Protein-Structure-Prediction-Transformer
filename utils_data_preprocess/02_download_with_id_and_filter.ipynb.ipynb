{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO1c5s6LyGhoEuXQcUnvbju"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ePN1RpUmXqcn"},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","from prody import fetchPDB, parsePDB, confProDy\n","from scipy.spatial.distance import pdist, squareform\n","from tqdm import tqdm\n","\n","confProDy(verbosity='none')\n","\n","DRIVE_PATH = '/content/drive/MyDrive/ProteinData/'\n","os.makedirs(DRIVE_PATH, exist_ok=True)\n","TXT_PATH = '/content/drive/MyDrive/ProteinData/immunoglobulin_ids.txt'\n","\n","BATCH_SIZE = 200\n","TOTAL_TARGET = 4000\n","length_stats = []\n","\n","def import_all_protein_ids(txt_path):\n","    \"\"\"Import all protein IDs from a file and shuffle them\"\"\"\n","    protein_ids = []\n","    with open(txt_path, 'r') as f:\n","        for line in f:\n","            protein_id = line.strip()\n","            if protein_id:\n","                pdb_id = protein_id.split('_')[0]\n","                protein_ids.append(pdb_id.lower())\n","    unique_ids = list(set(protein_ids))\n","    return unique_ids\n","\n","def has_chain_a(structure):\n","    \"\"\"Check if A chain is included\"\"\"\n","    chains = structure.select('protein').getChids()\n","    return 'A' in chains\n","\n","def process_single_pdb(pdb_id):\n","    \"\"\"Process and filter the PDB structure, extract only the A chain, and return (data, fail_reason)\"\"\"\n","    try:\n","        pdb_file = fetchPDB(pdb_id, compressed=False)\n","        structure = parsePDB(pdb_file)\n","\n","        if not has_chain_a(structure):\n","            return None, \"Missing A chain\"\n","\n","        # 只处理A链\n","        calphas = structure.select('protein and chain A and name CA')\n","\n","        if not calphas:\n","            return None, \"Missing CA atoms\"\n","\n","        num_residues = len(calphas)\n","\n","        residues = calphas.getResindices()\n","        if len(set(residues)) != (max(residues) - min(residues) + 1):\n","            return None, \"Residues are not continuous\"\n","\n","        coords = calphas.getCoords()\n","        seq = calphas.getSequence()\n","        dist_matrix = squareform(pdist(coords))\n","\n","        length_stats.append(num_residues)\n","\n","        return {\n","            'pdb_id': pdb_id,\n","            'sequence': ''.join(seq),\n","            'distance_matrix': dist_matrix,\n","            'coordinates': coords,\n","            'length': num_residues,\n","            'chain': 'A'\n","        }, None\n","\n","    except Exception as e:\n","        return None, f\"Exception Error: {e}\"\n","\n","def save_batch(processed_data, batch_index):\n","    \"\"\"Save a batch of processing results\"\"\"\n","    output_file = os.path.join(DRIVE_PATH, f'immunoglobulin_proteins_{batch_index + 1}.npz')\n","    np.savez_compressed(\n","        output_file,\n","        pdb_ids=[d['pdb_id'] for d in processed_data],\n","        sequences=[d['sequence'] for d in processed_data],\n","        distance_matrices=[d['distance_matrix'] for d in processed_data],\n","        coordinates=[d['coordinates'] for d in processed_data],\n","        lengths=[d['length'] for d in processed_data],\n","        chains=[d['chain'] for d in processed_data]\n","    )\n","    print(f\"Saved batch {batch_index + 1} with {len(processed_data)} proteins to {output_file}\")\n","\n","def print_length_statistics():\n","    \"\"\"Print length statistics\"\"\"\n","    if length_stats:\n","        min_length = min(length_stats)\n","        max_length = max(length_stats)\n","        avg_length = np.mean(length_stats)\n","        print(f\"   Minimum length: {min_length}\")\n","        print(f\"   Maximum length: {max_length}\")\n","        print(f\"   Average length: {avg_length:.1f}\")\n","        print(f\"   Total processed: {len(length_stats)}\")\n","\n","# -------------------- Main --------------------\n","\n","def process_all_batches():\n","    # Import and randomly shuffle all protein IDs\n","    protein_ids = import_all_protein_ids(TXT_PATH)\n","    processed_data = []\n","    batch_index = 0\n","    num_processed = 0\n","    with tqdm(total=TOTAL_TARGET, desc=\"Total Processed\") as pbar:\n","        for i, pdb_id in enumerate(protein_ids):\n","            if pbar.n >= TOTAL_TARGET:\n","                break\n","\n","            print(f\"Trying PDB ID: {pdb_id} ({i+1}/{len(protein_ids)})\")\n","\n","            data, fail_reason = process_single_pdb(pdb_id)\n","            if data:\n","                processed_data.append(data)\n","                pbar.update(1)\n","                num_processed += 1\n","                print(f\"Processed: {pdb_id} (#{num_processed}, length:{data['length']})\")\n","\n","                # After processing 100 pieces, save a batch\n","                if len(processed_data) == BATCH_SIZE:\n","                    save_batch(processed_data, batch_index)\n","                    batch_index += 1\n","                    processed_data = []\n","            else:\n","                print(f\"× Processing failed: {pdb_id}，reason: {fail_reason}\")\n","\n","        if processed_data:\n","            save_batch(processed_data, batch_index)\n","\n","    # 打印长度统计\n","    print_length_statistics()\n","    print(f\"\\n Processing completed! Successfully processed {num_processed}\")\n","\n","# -------------------- Run --------------------\n","if __name__ == \"__main__\":\n","    process_all_batches()"]}]}