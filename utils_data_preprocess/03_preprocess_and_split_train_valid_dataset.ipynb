{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN2k1FGIy9A85qelCYuOhZY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XwqM8WmGZG8L"},"outputs":[],"source":["import os\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import pickle\n","\n","def preprocess_protein_data():\n","    \"\"\"\n","    Data preprocessing main function:\n","    1. Load raw data and filter\n","    2. Calculate statistics\n","    3. Standardize data\n","    4. Split training and test sets\n","    5. Save processed data\n","    \"\"\"\n","    data_path = '/content/drive/MyDrive/ProteinData/immunoglobulin/'\n","    output_path = '/content/drive/MyDrive/ProteinData/immunoglobulin_processed/'\n","    os.makedirs(output_path, exist_ok=True)\n","\n","    # ------------------------- Step 1: Streaming and filtering data by length -------------------------\n","    print(\"Step 1: Streaming and filtering data by length...\")\n","    filtered_data = []\n","    total_count = 0\n","\n","    for i in range(1, 20):\n","        file_path = os.path.join(data_path, f'immunoglobulin_proteins_{i}.npz')\n","        print(f\"processig {i}/19: {file_path}\")\n","\n","        data = np.load(file_path, allow_pickle=True)\n","        sequences = data['sequences']\n","        matrices = data['distance_matrices']\n","        for seq, mat in zip(sequences, matrices):\n","            total_count += 1\n","            if 100 <= len(seq) <= 400:\n","                filtered_data.append((seq, mat))\n","\n","        del data, sequences, matrices\n","\n","    print(f\"Filtering completed: Total data {total_count} -> after filtering {len(filtered_data)}\")\n","\n","    # ------------------------- Step 2: Streaming global statistics -------------------------\n","    print(\"Step 2: Streaming global statistics...\")\n","\n","    def calculate_global_stats_streaming(data_list):\n","        sum_values = 0.0\n","        sum_squares = 0.0\n","        count = 0\n","\n","        for i, (seq, matrix) in enumerate(data_list):\n","            # Only take the upper triangular matrix\n","            upper_tri_indices = np.triu_indices_from(matrix, k=1)\n","            values = matrix[upper_tri_indices].astype(np.float64)\n","\n","            sum_values += np.sum(values)\n","            sum_squares += np.sum(values**2)\n","            count += len(values)\n","\n","        global_mean = sum_values / count\n","        global_std = np.sqrt(sum_squares / count - global_mean**2)\n","\n","        return global_mean, global_std\n","\n","    global_mean, global_std = calculate_global_stats_streaming(filtered_data)\n","    print(f\"Global Statistics - Mean: {global_mean:.6f}, Std: {global_std:.6f}\")\n","\n","    # ------------------------- Step 3: Data Verification -------------------------\n","    print(\"Step 3: Data Verification...\")\n","\n","    def validate_data_batch(data_list, sample_size=100):\n","        sample_indices = np.random.choice(len(data_list), min(sample_size, len(data_list)), replace=False)\n","\n","        for i in sample_indices:\n","            seq, mat = data_list[i]\n","            assert len(seq) == mat.shape[0] == mat.shape[1], f\"Data {i}: sequence length does not match matrix dimensions\"\n","            assert np.allclose(mat, mat.T, rtol=1e-10), f\"Data {i}: Matrix is ​​not symmetric\"\n","\n","        print(f\"Data verification passed! (Sampling verification of {len(sample_indices)} samples)\")\n","\n","    validate_data_batch(filtered_data)\n","\n","    # ------------------------- Step 4: Split the dataset -------------------------\n","    print(\"Step 4: Split the training and testing sets...\")\n","\n","    all_sequences = [item[0] for item in filtered_data]\n","    all_matrices = [item[1] for item in filtered_data]\n","\n","    train_seq, test_seq, train_mat, test_mat = train_test_split(\n","        all_sequences, all_matrices, test_size=0.2, random_state=42\n","    )\n","\n","    print(f\"Training set: {len(train_seq)} samples\")\n","    print(f\"Testing set: {len(test_seq)} samples\")\n","    del filtered_data, all_sequences, all_matrices\n","\n","    # ------------------------- Step 5: Recalculate training set statistics -------------------------\n","    print(\"Step 5: Recalculate training set statistics...\")\n","\n","    def calculate_train_stats(matrices):\n","        sum_values = 0.0\n","        sum_squares = 0.0\n","        count = 0\n","\n","        for i, matrix in enumerate(matrices):\n","            upper_tri_indices = np.triu_indices_from(matrix, k=1)\n","            values = matrix[upper_tri_indices].astype(np.float64)\n","\n","            sum_values += np.sum(values)\n","            sum_squares += np.sum(values**2)\n","            count += len(values)\n","\n","        train_mean = sum_values / count\n","        train_std = np.sqrt(sum_squares / count - train_mean**2)\n","\n","        return train_mean, train_std\n","\n","    train_global_mean, train_global_std = calculate_train_stats(train_mat)\n","    print(f\"Training set statistics - Mean: {train_global_mean:.6f}, Std: {train_global_std:.6f}\")\n","\n","    # ------------------------- Step 6: Standardize and save data -------------------------\n","    print(\"Step 6: Standardize and save data...\")\n","\n","    def standardize_distance_matrix(matrix, mean, std):\n","        return (matrix - mean) / std\n","\n","    print(\"Standardized training set...\")\n","    train_standardized_matrices = []\n","    for i, mat in enumerate(train_mat):\n","        standardized_mat = standardize_distance_matrix(mat, train_global_mean, train_global_std)\n","        train_standardized_matrices.append(standardized_mat)\n","\n","    print(\"Standardized test sets...\")\n","    test_standardized_matrices = []\n","    for i, mat in enumerate(test_mat):\n","        standardized_mat = standardize_distance_matrix(mat, train_global_mean, train_global_std)\n","        test_standardized_matrices.append(standardized_mat)\n","\n","    # ------------------------- Step 7: Save all data-------------------------\n","    print(\"Step 7: Save all data...\")\n","\n","    np.savez_compressed(\n","        os.path.join(output_path, 'train_data.npz'),\n","        sequences=train_seq,\n","        matrices=train_standardized_matrices\n","    )\n","\n","    np.savez_compressed(\n","        os.path.join(output_path, 'test_data.npz'),\n","        sequences=test_seq,\n","        matrices=test_standardized_matrices\n","    )\n","\n","    global_stats = {\n","        'train_mean': train_global_mean,\n","        'train_std': train_global_std,\n","        'global_mean': global_mean,\n","        'global_std': global_std,\n","        'train_samples': len(train_seq),\n","        'test_samples': len(test_seq),\n","        'total_samples': len(train_seq) + len(test_seq)\n","    }\n","\n","    with open(os.path.join(output_path, 'global_stats.pkl'), 'wb') as f:\n","        pickle.dump(global_stats, f)\n","\n","    np.savez(\n","        os.path.join(output_path, 'global_stats.npz'),\n","        **global_stats\n","    )\n","\n","    return global_stats\n","\n","# ------------------------- Run all-------------------------\n","if __name__ == \"__main__\":\n","    stats = preprocess_protein_data()"]}]}